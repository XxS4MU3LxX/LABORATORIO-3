# LABORATORIO #3

***Luz Marina Valderrama-5600741***

***Shesly Nicole Colorado - 5600756***

***Samuel Esteban Fonseca Luna - 5600808***

Este c칩digo aborda el problema del c칩ctel, donde se mezclan m칰ltiples fuentes de audio (como voces en un ambiente ruidoso) y se necesita separar la voz principal o fuente de inter칠s del ruido de fondo y otras voces, donde en este caso grabamos 3 audios de los tres integrantes del grupo hablablando al mismo tiempo.

Para lograrlo, se utiliza una t칠cnica de procesamiento de se침ales en 4 fases:

-  *Beamforming:* Enfocar la fuente principal de voz utilizando m칰ltiples micr칩fonos.
-  *ICA (FastICA):* Separar las fuentes mezcladas en componentes independientes.
 - *Normalizaci칩n:* Ajustar la amplitud de las se침ales para una mejor calidad de audio.
-  *Comparaci칩n de SNR (Signal-to-Noise Ratio):* Evaluar la calidad de la se침al filtrada en cada etapa.
 - *Selecci칩n y Guardado del Mejor Audio Filtrado:* Selecciona la se침al con el mejor SNR y la guarda como .wav para escuchar y evaluar la calidad de la voz separada.


# Librer칤as Necesarias
    import numpy as np
    import matplotlib.pyplot as plt
    import scipy.io.wavfile as wav
    from sklearn.decomposition import FastICA
    from scipy.linalg import svd
    import os

 `numpy:` Para operaciones matem치ticas y manipulaci칩n de matrices.
 `matplotlib.pyplot:` Para generar gr치ficas de SNR y an치lisis de se침ales.
 `scipy.io.wavfile:` Para leer y guardar archivos de audio .wav.
 `sklearn.decomposition.FastICA:` Para separaci칩n de fuentes mediante ICA.
 `scipy.linalg.svd:` Para Beamforming utilizando SVD (Singular Value Decomposition).
 `os:` Para manejo de directorios y guardar archivos.

# Definir carpeta de salida
    output_dir = r"C:\Users\Lenovo\.spyder-py3\jejeje"
    if not os.path.exists(output_dir):
    os.makedirs(output_dir)

Se define el directorio de salida donde se guardar치n:
- Audios filtrados y se침ales separadas.
- Resultados finales como gr치ficos y archivos .wav.
La variable output_dir contiene la ruta completa de la carpeta de salida.

 `os.path.exists(output_dir):` Verifica si la carpeta ya existe en el sistema de archivos.
 `os.makedirs(output_dir):` Si la carpeta no existe, la crea autom치ticamente.
Este enfoque previene errores si la carpeta ya existe y asegura que los archivos de salida se guarden correctamente.
    
# Cargar archivos de audio
    micL_path = 'micL.wav'
    micS_path = 'micS.wav'
    micSh_path = 'micSh.wav'
    noise_path = 'Sonido_ambiente.wav'


`micL.wav:` Se침al de micr칩fono izquierdo (Luz).

`micS.wav:` Se침al de micr칩fono central (Samuel).

`micSh.wav:` Se침al de micr칩fono derecho (Shesly).

`Sonido_ambiente.wav:` Ruido de fondo o sonido ambiente.

**쯇or qu칠 se utilizan m칰ltiples micr칩fonos?:**

Se utilizan m칰ltiples micr칩fonos para:
- Capturar la voz principal desde diferentes posiciones.
- Ayudar en la separaci칩n de fuentes utilizando diferencias espaciales.
- Mejorar la direccionalidad y reducir el ruido de fondo mediante Beamforming.
  
**쯈u칠 es el archivo de ruido ambiente?:**

Es una grabaci칩n del ruido de fondo sin ninguna fuente de voz principal. 

Se utiliza para calcular el SNR y comparar la calidad de las se침ales filtradas.

Tambi칠n se utiliza como referencia de ruido para evaluar la efectividad de la separaci칩n de fuentes.

# Leer archivos de audio
    sample_rate_L, data_L = wav.read(micL_path)
    sample_rate_S, data_S = wav.read(micS_path)
    sample_rate_Sh, data_Sh = wav.read(micSh_path)
    sample_rate_Noise, data_Noise = wav.read(noise_path)

Se leen los archivos de audio utilizando `wav.read()` de `scipy.io.wavfile`.

Cada archivo se descompone en dos partes:

`sample_rate_X:` La frecuencia de muestreo (en Hz), que indica cu치ntas muestras por segundo tiene la se침al de audio.

`data_X:` La se침al de audio en s칤, almacenada como un arreglo NumPy.

**쯇or qu칠 se utiliza .wav como formato de archivo?:**

.wav es un formato de audio sin p칠rdida (lossless), lo que significa que:
- No comprime ni modifica la se침al de audio original.
- Permite un procesamiento preciso sin p칠rdida de calidad.

# Recortar todas las se침ales al mismo tama침o m칤nimo
    min_length = min(len(data_L), len(data_S), len(data_Sh), len(data_Noise))
    data_L, data_S, data_Sh, data_Noise = data_L[:min_length], data_S[:min_length], data_Sh[:min_length], data_Noise[:min_length]

1. Encuentra la Longitud M칤nima:

`min_length = min(len(...)):` Encuentra la longitud m치s corta entre todas las se침ales `(data_L, data_S, data_Sh, data_Noise)`.

Esto asegura que ninguna se침al sea m치s larga que las dem치s.

2. Recorta Todas las Se침ales:

`Utiliza slicing (:)` para recortar cada se침al hasta la longitud m칤nima.

De esta manera, todas las se침ales terminan con exactamente el mismo n칰mero de muestras.

Este enfoque previene errores en etapas posteriores que requieren procesar las se침ales en paralelo.

**쯇or qu칠 se utiliza `min()` en lugar de `max()`?**

Si se utilizara `max()`, algunas se침ales ser칤an m치s cortas, lo que provocar칤a:
- Errores de Indexaci칩n al intentar acceder a 칤ndices inexistentes.
- P칠rdida de sincronizaci칩n en las se침ales durante el procesamiento conjunto.
- Distorsiones al aplicar algoritmos como ICA y Beamforming.

# Aplicar Beamforming con SVD
    signals_matrix = np.column_stack((data_L, data_S, data_Sh))
    U, S, Vt = svd(signals_matrix, full_matrices=False)
    beamformed_signals = U[:, :3] @ np.diag(S[:3])

**쯈u칠 es Beamforming y por qu칠 es importante?**

Beamforming es una t칠cnica utilizada para enfocar una fuente de sonido utilizando m칰ltiples micr칩fonos. 

Funciona aprovechando las diferencias de tiempo de llegada y diferencias de amplitud de la se침al en cada micr칩fono.

Permite:
- Enfocar la voz principal y reducir el ruido de fondo.
- Mejorar la direccionalidad al enfocarse en una fuente espec칤fica.
- Atenuar se침ales no deseadas provenientes de otras direcciones.
  
**쯇or qu칠 se utiliza SVD para Beamforming?**

SVD (Singular Value Decomposition) descompone una matriz en componentes principales, revelando:
- Direcciones dominantes de la se침al.
- Energ칤a y contribuci칩n de cada fuente en la mezcla.
SVD se utiliza aqu칤 para:
- Identificar las direcciones principales de llegada de la se침al de voz.
- Separar las se침ales mezcladas en componentes con m치xima energ칤a.

`np.column_stack(...)` concatena las se침ales de los 3 micr칩fonos en una sola matriz.

La matriz resultante `(signals_matrix)`

`svd()` descompone la matriz de se침ales en 3 componentes:

`U:` Matriz de direcciones principales de la se침al. Cada columna representa una direcci칩n en el espacio de mezclas.

`S:` Vector de valores singulares. Representa la energ칤a o intensidad de cada componente de la se침al.

`Vt:` Matriz de componentes fuente. Cada fila representa una fuente independiente en el espacio original.

`full_matrices=False` optimiza el c치lculo, manteniendo solo las componentes necesarias.

El producto de matrices `U[:, :3] @ np.diag(S[:3])` representa:
- La combinaci칩n 칩ptima de se침ales para enfocar la fuente principal.
- Una se침al que maximiza la relaci칩n se침al/ruido (SNR).
- Una se침al que aten칰a componentes de ruido provenientes de otras direcciones.
- 
# Aplicar ICA
    ica = FastICA(n_components=3, max_iter=3000, tol=0.0001, random_state=42)
    enhanced_signals = ica.fit_transform(beamformed_signals)
    
**쯇or qu칠 se utiliza FastICA en lugar de ICA est치ndar?**

FastICA es una versi칩n r치pida y optimizada de ICA que:
- Utiliza un algoritmo iterativo de m치xima verosimilitud.
- Maximiza la no gaussianidad de las se침ales separadas, usando kurtosis o entrop칤a negada.
- Es computacionalmente m치s eficiente y converge m치s r치pido que ICA est치ndar.
- Es ideal para se침ales de voz, que tienden a tener una distribuci칩n no gaussiana.



# Normalizar se침ales antes de guardar
    def normalize_signal(signal):
    signal = signal - np.mean(signal)
    max_val = np.max(np.abs(signal))
     if max_val > 0:
        signal = signal / max_val
        signal = (signal * 32767).astype(np.int16)
     return signal

    normalized_signals = np.apply_along_axis(normalize_signal, 0, enhanced_signals)

# Guardar se침ales normalizadas en archivos .wav
    output_files_normalized = {}
    for i in range(3):
     output_file = os.path.join(output_dir, f"enhanced_voice_final_{i+1}.wav")
     wav.write(output_file, sample_rate_L, normalized_signals[:, i])
     output_files_normalized[f"Voz Separada {i+1}"] = output_file

    print("Archivos generados:", output_files_normalized)

# Funci칩n para calcular SNR
    def calculate_snr(signal, noise):
     signal_power = np.mean(signal ** 2)
     noise_power = np.mean(noise ** 2)
      if noise_power > 0:
        return 10 * np.log10(signal_power / noise_power)
      else:
         return -np.inf  # Manejo de divisi칩n por cero

# C치lculo de SNR para las se침ales originales
    snr_original = {
      "MicL": calculate_snr(data_L, data_Noise),
      "MicS": calculate_snr(data_S, data_Noise),
      "MicSh": calculate_snr(data_Sh, data_Noise)
    }

# C치lculo de SNR para las se침ales despu칠s de Beamforming
    snr_beamforming = {}
    for i in range(beamformed_signals.shape[1]):
      snr_beamforming[f"Beamforming {i+1}"] = calculate_snr(beamformed_signals[:, i], data_Noise)

# C치lculo de SNR para las se침ales despu칠s de ICA
    snr_ica = {}
    for i in range(enhanced_signals.shape[1]):
      snr_ica[f"ICA {i+1}"] = calculate_snr(enhanced_signals[:, i], data_Noise)

# C치lculo de SNR para las se침ales Normalizadas
 snr_normalized = {}
    for i in range(normalized_signals.shape[1]):
      snr_normalized[f"Normalizado {i+1}"] = calculate_snr(normalized_signals[:, i], data_Noise)

# C치lculo de Potencia del Ruido Ambiente
    potencia_ruido_ambiente = 10 * np.log10(np.mean(data_Noise ** 2))

# Imprimir Potencia del Ruido Ambiente
    print("\n=== Potencia del Ruido Ambiente ===")
    print(f"Potencia de Ruido: {potencia_ruido_ambiente:.2f} dB")

# Imprimir resultados en consola
    print("\n=== SNR de las Se침ales ===")
    print("Original:", snr_original)
    print("Despu칠s de Beamforming:", snr_beamforming)
    print("Despu칠s de ICA:", snr_ica)
    print("Despu칠s de Normalizaci칩n:", snr_normalized)

# === Selecci칩n y Guardado del Audio Filtrado Final ===

# Funci칩n para seleccionar la se침al con el mejor SNR
    def select_best_snr_signal(snr_dict, signals_matrix):
      best_snr_key = max(snr_dict, key=snr_dict.get)
      best_index = int(best_snr_key.split()[-1]) - 1  # Extraer el 칤ndice de la mejor se침al
      best_signal = signals_matrix[:, best_index]
      print(f"\n=== Mejor Se침al Filtrada ===")
      print(f"Se침al Seleccionada: {best_snr_key} con SNR de {snr_dict[best_snr_key]:.2f} dB")
      return best_signal

# Seleccionar la mejor se침al normalizada
    best_filtered_signal = select_best_snr_signal(snr_normalized, normalized_signals)

# Guardar el archivo de audio filtrado final
    output_file_filtered = os.path.join(output_dir, "filtered_voice_final.wav")
    wav.write(output_file_filtered, sample_rate_L, best_filtered_signal.astype(np.int16))

    print(f"\n=== Archivo de Audio Filtrado Guardado ===")
    print(f"游닌 Archivo Generado: {output_file_filtered}")

# Graficar SNR en diferentes etapas
    def plot_snr_comparison(snr_dicts, titles, potencia_ruido):
     plt.figure(figsize=(14, 12))
      for i, (snr_dict, title) in enumerate(zip(snr_dicts, titles)):
         labels = list(snr_dict.keys())
         values = list(snr_dict.values())
        
         plt.subplot(3, 2, i+1)
         plt.bar(labels, values, color='skyblue', label="SNR Se침ales")
         plt.axhline(y=potencia_ruido, color='red', linestyle='--', label="Potencia Ruido Ambiente")
         plt.title(title)
         plt.xlabel("Se침ales")
         plt.ylabel("SNR (dB)")
         plt.xticks(rotation=45)
         plt.grid(True, linestyle='--', alpha=0.7)
         plt.legend()
    
      plt.tight_layout()
      plt.show()

    plot_snr_comparison(
      [snr_original, snr_beamforming, snr_ica, snr_normalized],
      ["Original", "Beamforming", "ICA", "Normalizaci칩n"],
      potencia_ruido_ambiente
    )







# Guardar en diccionarios para facilitar el acceso
audio_data = {"micL": data_L, "micS": data_S, "micSh": data_Sh}
sample_rates = {"micL": sample_rate_L, "micS": sample_rate_S, "micSh": sample_rate_Sh}

# Calcular la Relaci칩n Se침al-Ruido (SNR)
def calculate_snr(signal, noise):
    signal_power = np.mean(signal ** 2)
    noise_power = np.mean(noise ** 2)
    return 10 * np.log10(signal_power / noise_power)

snr_values = {key: calculate_snr(data, data_Noise[:len(data)]) for key, data in audio_data.items()}
print("Valores de SNR:", snr_values)

# Visualizaci칩n de las formas de onda y espectrogramas con subplots
fig = plt.figure(figsize=(15, 15))
for i, (key, data) in enumerate(audio_data.items()):
    sr = sample_rates[key]
    time = np.linspace(0, len(data) / sr, num=len(data))

    # Forma de onda
    plt.subplot(len(audio_data), 2, i * 2 + 1)
    plt.plot(time, data)
    plt.title(f"Forma de onda - {key}")
    plt.xlabel("Tiempo (s)")
    plt.ylabel("Amplitud")

    # Espectrograma
    f, t, Sxx = signal.spectrogram(data, sr)
    plt.subplot(len(audio_data), 2, i * 2 + 2)
    plt.imshow(10 * np.log10(Sxx), aspect='auto', origin='lower',
               extent=[t.min(), t.max(), f.min(), f.max()])
    plt.title(f"Espectrograma - {key}")
    plt.xlabel("Tiempo (s)")
    plt.ylabel("Frecuencia (Hz)")

plt.tight_layout()
plt.show()

# Aplicar Beamforming con SVD
min_length = min(len(data_L), len(data_S), len(data_Sh))
signals_matrix = np.column_stack((data_L[:min_length], data_S[:min_length], data_Sh[:min_length]))
print("Dimensiones correctas de signals_matrix:", signals_matrix.shape)

# Visualizar se침ales originales alineadas antes de aplicar SVD
plt.figure(figsize=(15, 10))
for i in range(3):
    plt.subplot(3, 1, i + 1)
    plt.plot(signals_matrix[:, i])
    plt.title(f"Se침al Original Alineada - Mic {i+1}")
    plt.xlabel("Muestras")
    plt.ylabel("Amplitud")
plt.tight_layout()
plt.show()

# Aplicar SVD correctamente y mostrar las dimensiones de U, S y Vt
U, S, Vt = svd(signals_matrix, full_matrices=False)
print("Dimensiones de U:", U.shape)
print("Dimensiones de S:", S.shape)
print("Dimensiones de Vt:", Vt.shape)

# Verificar los valores singulares
print("Valores singulares (S):", S[:10])  # Mostrar solo los primeros 10 valores singulares

# Reconstruir beamformed_signals con m치s componentes
beamformed_signals = U[:, :2] @ np.diag(S[:2])
print("Dimensiones de beamformed_signals (ajustado):", beamformed_signals.shape)

# Visualizar las se침ales despu칠s de Beamforming
plt.figure(figsize=(15, 10))
for i in range(2):
    plt.subplot(2, 1, i + 1)
    plt.plot(beamformed_signals[:, i])
    plt.title(f"Beamformed Signal - Componente {i+1}")
    plt.xlabel("Muestras")
    plt.ylabel("Amplitud")
plt.tight_layout()
plt.show()

# Aplicar Filtro de Wiener para reducci칩n de ruido
denoised_signals = np.apply_along_axis(lambda x: wiener(x, mysize=29), 0, beamformed_signals)

# Visualizaci칩n de las se침ales procesadas con subplots
plt.figure(figsize=(12, 6))
for i in range(2):
    plt.subplot(2, 1, i + 1)
    plt.plot(denoised_signals[:, i])
    plt.title(f"Se침al Filtrada con Wiener - Componente {i+1}")
    plt.xlabel("Muestras")
    plt.ylabel("Amplitud")

plt.tight_layout()
plt.show()

# Aplicar PCA para estabilizar las se침ales
pca = PCA(n_components=2, whiten=True, random_state=42)
pca_signals = pca.fit_transform(denoised_signals)

# Aplicar ICA para separaci칩n de fuentes
ica = FastICA(n_components=2, max_iter=3000, tol=0.0001, random_state=42)
enhanced_signals = ica.fit_transform(pca_signals)

# Visualizaci칩n de las se침ales separadas con subplots
plt.figure(figsize=(12, 6))
for i in range(2):
    plt.subplot(2, 1, i + 1)
    plt.plot(enhanced_signals[:, i])
    plt.title(f"Se침al Separada por ICA - Componente {i+1}")
    plt.xlabel("Muestras")
    plt.ylabel("Amplitud")

plt.tight_layout()
plt.show()

# Funci칩n para normalizar se침ales
def normalize_signal(signal):
    signal = signal - np.mean(signal)  # Eliminar DC
    max_val = np.max(np.abs(signal))
    if max_val > 0:
        signal = signal / max_val  # Normalizar a [-1, 1]
        signal = (signal * 32767).astype(np.int16)  # Escalar a int16
    return signal

# Normalizar las se침ales antes de guardarlas
normalized_signals = np.apply_along_axis(normalize_signal, 0, enhanced_signals)

# Guardar las se침ales separadas normalizadas
output_files_normalized = {}
for i in range(2):
    output_file = f"enhanced_voice_final_{i+1}.wav"
    wav.write(output_file, sample_rate_L, normalized_signals[:, i])
    output_files_normalized[f"Voz Separada Final Normalizada {i+1}"] = output_file

# Mostrar los archivos generados
print("Archivos de audio generados:", output_files_normalized)
